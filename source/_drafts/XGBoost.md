---
title: "\x96\x96XGBoost"
tags:
---
# GBDT
GBDT的精髓在于训练的时候都是以上一颗树的残差为目标，这个残差就是上一个树的预测值与真实值的差值。

    比如，当前样本年龄是18岁，那么第一颗会去按18岁来训练，但是训练完之后预测的年龄为12岁，差值为6，
    所以第二颗树的会以6岁来进行训练，假如训练完之后预测出来的结果为6，那么两棵树累加起来就是真实年龄了，
    但是假如第二颗树预测出来的结果是5，那么剩余的残差1就会交给第三个树去训练。
Boosting的好处就是每一步的参加就是变相了增加了分错instance的权重，而对已经对的instance趋向于0，这样后面的树就可以更加关注错分的instance的训练了

## Shrinkage
Shrinkage认为，每次走一小步逐步逼近的结果要比每次迈一大步逼近结果更加容易避免过拟合。
[Math Processing Error]
就像我们做互联网，总是先解决60%用户的需求凑合着，再解决35%用户的需求，最后才关注那5%人的需求，这样就能逐渐把产品做好.

调参
树的个数 100~10000
叶子的深度 3~8
学习速率 0.01~1
叶子上最大节点树 20
训练采样比例 0.5~1
训练特征采样比例 sqrt(num)
## 优缺点：
**优点：**
 1. 精度高
 2. 能处理非线性数据
 3. 能处理多特征类型
 4. 适合低维稠密数据
**缺点：**
 1. 并行麻烦（因为上下两颗树有联系）
 2. 多分类的时候 复杂度很大

XGBoost行列抽样？
XGBoost特征重要程度，你觉得它这样做合理吗？
XGBoost连续值你为什么要分箱？


[XGBoost: A Scalable Tree Boosting System](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)
