---
title: Hexo搭建博客进阶之SEO搜索引擎优化
date: 2018-12-12 10:47:35
tags: Hexo
categories: Tools
---
SEO (Search Engine Optimization)，即搜索引擎优化。简单来说，SEO就是您可以使用提升网站排名的所有方法的总称，SEO用于确保您的网站及其内容在搜索引擎结果页面（SERP）上的可见性。

<!--more-->

<!-- # 验证你的网站(让你博客被搜索引擎找到) -->

**查看你的博客是否被收录**

在谷歌或者百度的搜索链接中，使用以下格式可以直接搜索自己的域名， 如果能搜索到就说明已经被收录，反之则没有。可以直接搜索自己的域名，或者加一些关键词来更好地判断，例如：`site: https://fuhailin.github.io/`

若未被搜索引擎收录，则需进行以下配置，

## 第一步：网站所有权验证

### Google

首先要让搜索引擎先验证我们对网站的所有权，Google搜索引擎提交的入口为：
[Google Search Console](https://search.google.com/search-console)


验证你对该网站的所有权有多种方式，我这里使用的是`Hexo->Next`主题中提供的"google_analytics"验证方式

在`主题配置文件`中可以找到google_analytics字段，到[Google Analytics](https://analytics.google.com/analytics/web)中生成你自己的 google_analytics 代码，将这个代码粘贴到`主题配置文件`中 google_analytics 字段后面，像这样：

![](TIM20181212110446.png)

我的就已经验证成功了：
![](TIM20181212105705.png)
<!-- <img src="TIM20181212105705.png" width="50%" height="50%" title="所有权验证." alt=""/> -->

### 百度

我这里选用的验证方式是通过文件验证：

![](TIM20181212140519.png)

将下载的文件`baidu_verify_evCLN0THH4.html`放置到`source`文件夹，然后重新编译博客`hexo g`，会发现刚刚添加的文件复制到了`themes/next/source`文件夹中，通过`hexo d`完成部署之后，可以通过域名访问刚刚的链接了：https://fuhailin.github.io/baidu_verify_DWdJWj7U7R.html ，点击完成验证即可。

## 第二步：添加站点地图

### 生成sitemap文件

安装相关插件
```
npm install hexo-generator-sitemap --save
npm install hexo-generator-baidu-sitemap --save
```
修改配置：修改`站点配置文件`_config.yml, 添加如下两段配置
```
# hexo sitemap
sitemap:
  path: sitemap.xml

baidusitemap:
  path: baidusitemap.xml
```

然后重新编译你的博客：`hexo g`，看看在public文件夹里面是不是出现了`sitemap.xml`和`baidusitemap.xml`两个文件，你还可以本地访问 http://localhost:4000/sitemap.xml 和 http://localhost:4000/baidusitemap.xml 查看效果，说明配置成功。

## 第三步：搜索引擎收录

### Google

谷歌操作比较简单，就是向[Google Search Console](https://search.google.com/search-console)提交sitemap：
![](TIM20181212104327.png)

### 百度

百度收录很麻烦，效率比谷歌差远了。由于 GitHub 屏蔽了百度的爬虫，即使提交成功，百度知道这里有可供抓取的链接，也不一定能抓取成功。我们还需要设置自动推送。

**设置自动推送**
在主题配置文件下设置,将baidu_push设置为true：
```
# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEO
baidu_push: true
```

## 其他优化

### 文章链接优化

Hexo 默认的文章链接形式为 `domain/year/month/day/postname`，分级较多，造成 URL(Universal Resource Locator，统一资源定位符) 较长，不利于搜索引擎检索，我们可根据需要缩短 URL，将其改为 `domain/postname`的形式。编辑**站点配置文件_config.yml**, 将 permalink 字段改为 `permalink: :title/`。
tips:
如果仍想在 URL 中保留时间要素，比如将 URL 改为 `domain/year-month-day/postname` 的形式，也可将permalink字段改为 `permalink: :year-:month-:day/:title/`


### 添加蜘蛛协议robots.txt

> robots.txt（统一小写）是一种存放于网站根目录下的 ASCII 编码的文本文件，它通常告诉网络搜索引擎的漫游器（又称网络蜘蛛），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的。

在根目录 **source** 文件下新建 **robots.txt** 文件，添加以下文件内容（将 Sitemap 中的域名切换成自己网站域名）
```
User-agent: *
Allow: /
Allow: /archives/
Allow: /tags/
Allow: /categories/
Allow: /about/

Disallow: /vendors/
Disallow: /js/
Disallow: /css/
Disallow: /fonts/
Disallow: /vendors/
Disallow: /fancybox/

Sitemap: https://fuhailin.github.io/sitemap.xml
Sitemap: https://fuhailin.github.io/baidusitemap.xml
```
> 参数说明： User-agent: * 允许所有 robot 访问，Allow 允许访问 X 目录，Disallow 禁止访问 X 目录



**References**:
  1. [Hexo 优化：提交 sitemap 及解决百度爬虫无法抓取 GitHub Pages 链接问题](http://www.yuan-ji.me/Hexo-%E4%BC%98%E5%8C%96%EF%BC%9A%E6%8F%90%E4%BA%A4sitemap%E5%8F%8A%E8%A7%A3%E5%86%B3%E7%99%BE%E5%BA%A6%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96-GitHub-Pages-%E9%97%AE%E9%A2%98/)
