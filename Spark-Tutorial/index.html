<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/32x32-logo.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/16x16-logo.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"fuhailin.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="Spark学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark入门笔记—基本概念与单机环境配置">
<meta property="og:url" content="https://fuhailin.github.io/Spark-Tutorial/index.html">
<meta property="og:site_name" content="赵大寳">
<meta property="og:description" content="Spark学习笔记">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-11.01.45-PM.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-18-01-05.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.24.52-PM.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-21-27-07.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.00.24-PM.png">
<meta property="og:image" content="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vZnVoYWlsaW4vT2JqZWN0LVN0b3JhZ2UtU2VydmljZS9yYXcvbWFzdGVyL3dlY2hhdF9jaGFubmVsLnBuZw?x-oss-process=image/format,png">
<meta property="article:published_time" content="2019-04-25T10:25:02.000Z">
<meta property="article:modified_time" content="2021-07-23T06:42:04.639Z">
<meta property="article:author" content="赵大寳">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-11.01.45-PM.png">


<link rel="canonical" href="https://fuhailin.github.io/Spark-Tutorial/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://fuhailin.github.io/Spark-Tutorial/","path":"Spark-Tutorial/","title":"Spark入门笔记—基本概念与单机环境配置"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Spark入门笔记—基本概念与单机环境配置 | 赵大寳</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129037882-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-129037882-1","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?d0967e9b160f4f6248804003642ee818"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">赵大寳</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">鶸鸡程序员，新世纪农民工</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
        <li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
        <li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E4%B8%AD%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.</span> <span class="nav-text">Spark中的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-Shell"><span class="nav-number">1.1.</span> <span class="nav-text">Spark Shell</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-Resilient-Distributed-Dataset"><span class="nav-number">1.2.</span> <span class="nav-text">RDD(Resilient Distributed Dataset)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E4%B8%AD%E7%9A%84%E7%BB%84%E4%BB%B6"><span class="nav-number">1.3.</span> <span class="nav-text">Spark中的组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Core"><span class="nav-number">1.3.1.</span> <span class="nav-text">Spark Core</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">1.3.2.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">1.3.3.</span> <span class="nav-text">Spark SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GraphX"><span class="nav-number">1.3.4.</span> <span class="nav-text">GraphX</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLlib"><span class="nav-number">1.3.5.</span> <span class="nav-text">MLlib</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E7%9A%84%E5%AE%89%E8%A3%85"><span class="nav-number">2.</span> <span class="nav-text">Spark的安装</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85JDK%E4%B8%8EHadoop"><span class="nav-number">2.1.</span> <span class="nav-text">安装JDK与Hadoop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Spark"><span class="nav-number">2.2.</span> <span class="nav-text">安装Spark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Spark-Shell-%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="nav-number">2.3.</span> <span class="nav-text">使用 Spark Shell 编写代码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BDtext%E6%96%87%E4%BB%B6"><span class="nav-number">2.3.1.</span> <span class="nav-text">加载text文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PySpark%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%BC%96%E7%A8%8B"><span class="nav-number">3.</span> <span class="nav-text">PySpark独立应用程序编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E8%BF%87-spark-submit-%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">通过 spark-submit 运行程序</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scala-on-Spark%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%BC%96%E7%A8%8B"><span class="nav-number">3.0.1.</span> <span class="nav-text">Scala on Spark独立应用编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%AE%89%E8%A3%85sbt"><span class="nav-number">3.0.1.1.</span> <span class="nav-text">1. 安装sbt</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Scala%E7%BC%96%E7%A0%81"><span class="nav-number">3.0.1.2.</span> <span class="nav-text">2. Scala编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8-sbt-%E6%89%93%E5%8C%85-Scala-%E7%A8%8B%E5%BA%8F"><span class="nav-number">3.0.1.3.</span> <span class="nav-text">3. 使用 sbt 打包 Scala 程序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E9%80%9A%E8%BF%87-spark-submit-%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F"><span class="nav-number">3.0.1.4.</span> <span class="nav-text">4. 通过 spark-submit 运行程序</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Java-on-Spark%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%BC%96%E7%A8%8B"><span class="nav-number">3.0.2.</span> <span class="nav-text">Java on Spark独立应用编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%AE%89%E8%A3%85maven"><span class="nav-number">3.0.2.1.</span> <span class="nav-text">1. 安装maven</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8-maven-%E6%89%93%E5%8C%85-Java-%E7%A8%8B%E5%BA%8F"><span class="nav-number">3.0.2.2.</span> <span class="nav-text">3. 使用 maven 打包 Java 程序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E9%80%9A%E8%BF%87-spark-submit-%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F-1"><span class="nav-number">3.0.2.3.</span> <span class="nav-text">4. 通过 spark-submit 运行程序</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="赵大寳"
      src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/uploads-avatar.jpg">
  <p class="site-author-name" itemprop="name">赵大寳</p>
  <div class="site-description" itemprop="description">赵大寳個人小站</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fuhailin" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fuhailin" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hailinfufu@outlook.com" title="E-Mail → mailto:hailinfufu@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/%E6%B5%B7%E6%9E%97-%E4%BB%98-855633ab/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;%E6%B5%B7%E6%9E%97-%E4%BB%98-855633ab&#x2F;" rel="noopener" target="_blank"><i class="linkedin fa-fw"></i>Linkedin</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/u010412858" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;u010412858" rel="noopener" target="_blank"><i class="csdn fa-fw"></i>CSDN</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://tcxx.info/" title="https:&#x2F;&#x2F;tcxx.info&#x2F;" rel="noopener" target="_blank">甜欣屋</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://licstar.net/" title="http:&#x2F;&#x2F;licstar.net&#x2F;" rel="noopener" target="_blank">Siwei Lai的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wepon.me/" title="http:&#x2F;&#x2F;wepon.me&#x2F;" rel="noopener" target="_blank">wepon的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://jd92.wang/" title="http:&#x2F;&#x2F;jd92.wang&#x2F;" rel="noopener" target="_blank">迁移学习王晋东</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.yinwang.org/" title="http:&#x2F;&#x2F;www.yinwang.org&#x2F;" rel="noopener" target="_blank">王垠的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://blog.sina.com.cn/weiyanzheng" title="http:&#x2F;&#x2F;blog.sina.com.cn&#x2F;weiyanzheng" rel="noopener" target="_blank">魏延政的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://iphysresearch.github.io/" title="https:&#x2F;&#x2F;iphysresearch.github.io&#x2F;" rel="noopener" target="_blank">IPhysResearch</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://paperswithcode.com/task/recommendation-systems" title="https:&#x2F;&#x2F;paperswithcode.com&#x2F;task&#x2F;recommendation-systems" rel="noopener" target="_blank">Papers with Code</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://jcf94.com/" title="http:&#x2F;&#x2F;jcf94.com&#x2F;" rel="noopener" target="_blank">Chenfan Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://coolshell.cn/" title="https:&#x2F;&#x2F;coolshell.cn&#x2F;" rel="noopener" target="_blank">左耳朵耗子</a>
        </li>
    </ul>
  </div>

          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/fuhailin" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://fuhailin.github.io/Spark-Tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/uploads-avatar.jpg">
      <meta itemprop="name" content="赵大寳">
      <meta itemprop="description" content="赵大寳個人小站">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="赵大寳">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark入门笔记—基本概念与单机环境配置
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-04-25 18:25:02" itemprop="dateCreated datePublished" datetime="2019-04-25T18:25:02+08:00">2019-04-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-07-23 14:42:04" itemprop="dateModified" datetime="2021-07-23T14:42:04+08:00">2021-07-23</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

            <div class="post-description">Spark学习笔记</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文通过收集Spark中的基本概念、在Mac上配置伪分布式环境并分别用Python、Scala、Java三种语言独立编程实现了分布式版的WordCount程序以进行测试学习，来熟悉Spark的常用操作。</p>
<span id="more"></span>

<h1 id="Spark中的基本概念"><a href="#Spark中的基本概念" class="headerlink" title="Spark中的基本概念"></a>Spark中的基本概念</h1><h2 id="Spark-Shell"><a href="#Spark-Shell" class="headerlink" title="Spark Shell"></a>Spark Shell</h2><p>Spark的shell提供了一个简单的API可供学习, 其也是一个用于分析数据的强有力交互工具。</p>
<h2 id="RDD-Resilient-Distributed-Dataset"><a href="#RDD-Resilient-Distributed-Dataset" class="headerlink" title="RDD(Resilient Distributed Dataset)"></a>RDD(Resilient Distributed Dataset)</h2><p>RDD（Resilient Distributed Dataset）叫做<strong>弹性分布式数据集</strong>，是Spark中最基本的数据结构。它是一个不可变的分布式对象集合。在RDD中的每一个数据集被划分进逻辑分区，不同的部分将在集群的不同节点上进行计算。RDD能够包含任意类型的对象，包括Python、Java、Scala甚至用户自定义类型。</p>
<h2 id="Spark中的组件"><a href="#Spark中的组件" class="headerlink" title="Spark中的组件"></a>Spark中的组件</h2><p>Spark组件使Apache Spark快速和可靠。为了解决使用Hadoop MapReduce时出现的问题，很多Spark组件被构建出来。 Apache Spark具有以下组件：</p>
<ol>
<li>Spark Core</li>
<li>Spark Streaming</li>
<li>Spark SQL</li>
<li>GraphX</li>
<li>MLlib (Machine Learning)</li>
</ol>
<h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><p><em>Spark Core</em>是大规模并行计算和分布式数据处理的基本引擎。它负责：</p>
<ol>
<li>内存管理和故障恢复</li>
<li>在群集上调度，分发和监视作业</li>
<li>与存储系统交互</li>
</ol>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p><em>Spark Streaming</em> 是Spark用于处理实时流数据的组件。它支持实时数据流的高吞吐量和容错流处理。基本流单元是 <em>DStream</em> ，其基本上是一系列用于处理实时数据的RDD（弹性分布式数据集）。</p>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p><em>Spark SQL</em> 是Spark中的一个新模块，它将关系处理与Spark的函数式编程API集成在一起。 它支持通过SQL或Hive查询语言查询数据。以下是Spark SQL的四个库：</p>
<ol>
<li>Data Source API</li>
<li>DataFrame API</li>
<li>Interpreter &amp; Optimizer</li>
<li>SQL Service</li>
</ol>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p><em>GraphX</em>是用于图形和图形并行计算的Spark API。 因此，它使用弹性分布式属性图(Resilient Distributed Property Graph)扩展了Spark RDD。</p>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><p><em>MLlib</em> 代表机器学习库Machine Learning Library。Spark MLlib用于在Apache Spark中执行机器学习。</p>
<h1 id="Spark的安装"><a href="#Spark的安装" class="headerlink" title="Spark的安装"></a>Spark的安装</h1><p>Spark可以独立安装使用，也可以和Hadoop一起安装使用。这里我们采用和Hadoop一起安装使用，这样就可以让Spark使用HDFS存取数据。需要说明的是，当安装好Spark以后，里面就自带了Scala环境，不需要额外安装Scala.</p>
<p>本教程的具体运行环境如下：</p>
<ul>
<li>Hadoop 3.1.3</li>
<li>Java JDK 1.8</li>
<li>Spark 2.4.5</li>
</ul>
<h2 id="安装JDK与Hadoop"><a href="#安装JDK与Hadoop" class="headerlink" title="安装JDK与Hadoop"></a>安装JDK与Hadoop</h2><p><a href="https://fuhailin.github.io/Hadoop-on-MacOS/">https://fuhailin.github.io/Hadoop-on-MacOS/</a></p>
<h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><p>由于已经安装了Hadoop，所以，在“Choose a package type”后面需要选择“Pre-build with user-provided Hadoop [can use with most Hadoop distributions]”，然后，点击“Download Spark”后面的“spark-2.4.5-bin-without-hadoop.tgz”下载即可。<br>Spark部署模式主要有四种：</p>
<ul>
<li>Local模式（单机模式）</li>
<li>Standalone模式（使用Spark自带的简单集群管理器）</li>
<li>YARN模式（使用YARN作为集群管理器）</li>
<li>Mesos模式（使用Mesos作为集群管理器）</li>
</ul>
<p>这里介绍Local模式（单机模式）的 Spark安装。我们选择Spark 2.4.5 版本，并且假设当前使用用户名hadoop登录了Linux操作系统(MacOS可忽略这一步操作)。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf spark-2.4.2-bin-without-hadoop.tgz -C /usr/<span class="built_in">local</span>/</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span></span><br><span class="line">sudo mv ./spark-2.4.2-bin-without-hadoop/ ./spark</span><br><span class="line">sudo chown -R hadoop:hadoop ./spark          <span class="comment"># 此处的 hadoop 为你的用户名，MacOS可忽略</span></span><br></pre></td></tr></table></figure>
<p>安装后，还需要修改Spark的配置文件spark-env.sh</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/spark</span><br><span class="line">cp ./conf/spark-env.sh.template ./conf/spark-env.sh</span><br></pre></td></tr></table></figure>
<p>编辑spark-env.sh文件(vim ./conf/spark-env.sh)，添加你的Hadoop配置信息:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br></pre></td></tr></table></figure>
<p>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。然后通过如下命令，修改环境变量<code>vim ~/.bashrc</code>，在.bashrc文件中添加如下内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SPARK CONFIG</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$SPARK_HOME</span>/python:<span class="variable">$SPARK_HOME</span>/python/lib/py4j-0.10.7-src.zip:<span class="variable">$PYTHONPATH</span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=python3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<p>PYTHONPATH环境变量主要是为了在Python3中引入pyspark库，PYSPARK_PYTHON变量主要是设置pyspark运行的python版本。<br>.bashrc中必须包含<code>JAVA_HOME</code>,<code>HADOOP_HOME</code>,<code>SPARK_HOME</code>,<code>PYTHONPATH</code>,<code>PYSPARK_PYTHON</code>,<code>PATH</code>这些环境变量。如果已经设置了这些变量则不需要重新添加设置。<br><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-11.01.45-PM.png" alt="SPARK_HOME环境变量"><br>接着还需要让该环境变量生效，执行<code>source ~/.bashrc</code>。<br>配置完成后就可以直接使用，不需要像Hadoop运行启动命令。<br>通过运行Spark自带的示例，验证Spark是否安装成功。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run-example SparkPi</span><br></pre></td></tr></table></figure>
<p>执行时会输出非常多的运行信息，输出结果不容易找到，可以通过 grep 命令进行过滤（命令中的 2&gt;&amp;1 可以将所有的信息都输出到 stdout 中，否则由于输出日志的性质，还是会输出到屏幕中）:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run-example SparkPi 2&gt;&amp;1 | grep <span class="string">&quot;Pi is&quot;</span></span><br></pre></td></tr></table></figure>
<p>过滤后的运行结果如下图示，<br><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-18-01-05.png" alt="run-example SparkPi"></p>
<h2 id="使用-Spark-Shell-编写代码"><a href="#使用-Spark-Shell-编写代码" class="headerlink" title="使用 Spark Shell 编写代码"></a>使用 Spark Shell 编写代码</h2><p><strong>启动Spark Shell</strong>：spark-shell，启动spark-shell后，会自动创建名为sc的SparkContext对象和名为spark的SparkSession对象,如图：</p>
<p><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.24.52-PM.png" alt="Screen-Shot-2020-03-08-at-10.24.52-PM"></p>
<h3 id="加载text文件"><a href="#加载text文件" class="headerlink" title="加载text文件"></a>加载text文件</h3><p>spark创建sc，可以加载本地文件和HDFS文件创建RDD。这里用Spark自带的本地文件README.md文件测试。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">&quot;file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>加载HDFS文件和本地文件都是使用textFile，区别是添加前缀(<code>hdfs://</code>和<code>file:///</code>)进行标识。</p>
<h1 id="PySpark独立应用程序编程"><a href="#PySpark独立应用程序编程" class="headerlink" title="PySpark独立应用程序编程"></a>PySpark独立应用程序编程</h1><p>学习Spark程序开发，建议首先通过pyspark交互式学习，加深Spark程序开发的理解。<br>PySpark提供了简单的方式来学习 API，并且提供了交互的方式来分析数据。你可以输入一条语句，PySpark会立即执行语句并返回结果，这就是我们所说的REPL（Read-Eval-Print Loop，交互式解释器），为我们提供了交互式执行环境，表达式计算完成就会输出结果，而不必等到整个程序运行完毕，因此可即时查看中间结果，并对程序进行修改，这样可以在很大程度上提升开发效率。</p>
<p>前面已经安装了Hadoop和Spark，如果Spark不使用HDFS和YARN，那么就不用启动Hadoop也可以正常使用Spark。如果在使用Spark的过程中需要用到 HDFS，就要首先启动 Hadoop（启动Hadoop的方法可以参考上面给出的<a href="https://fuhailin.github.io/Hadoop-Install/">Hadoop安装教程</a>）。<br>这里假设不需要用到HDFS，因此，就没有启动Hadoop。现在我们直接开始使用Spark。</p>
<p>注意：如果按照上面的安装步骤，已经设置了PYSPARK_PYTHON环境变量，那么你直接使用如下命令启动pyspark即可。<br><code>pyspark</code><br>如果没有设置PYSPARK_PYTHON环境变量，则使用如下命令启动pyspark</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PYSPARK_PYTHON=python3</span><br><span class="line">pyspark</span><br></pre></td></tr></table></figure>
<p>pyspark命令及其常用的参数如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master &lt;master-url&gt;</span><br></pre></td></tr></table></figure>
<p>Spark的运行模式取决于传递给SparkContext的Master URL的值。Master URL可以是以下任一种形式：</p>
<ul>
<li>local 使用一个Worker线程本地化运行SPARK(完全不并行)</li>
<li>local[*] 使用逻辑CPU个数数量的线程来本地化运行Spark</li>
<li>local[K] 使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU核数设定）</li>
<li>spark://HOST:PORT 连接到指定的Spark standalone master。默认端口是7077.</li>
<li>yarn-client 以客户端模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</li>
<li>yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</li>
<li>mesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050。</li>
</ul>
<p>需要强调的是，这里我们采用“本地模式”（local）运行Spark，关于如何在集群模式下运行Spark，可以参考后面的“在集群上运行Spark应用程序”。<br>在Spark中采用本地模式启动pyspark的命令主要包含以下参数：<br>–master：这个参数表示当前的pyspark要连接到哪个master，如果是local[*]，就是使用本地模式启动pyspark，其中，中括号内的星号表示需要使用几个CPU核心(core)；<br>–jars： 这个参数用于把相关的JAR包添加到CLASSPATH中；如果有多个jar包，可以使用逗号分隔符连接它们；</p>
<p>比如，要采用本地模式，在4个CPU核心上运行pyspark：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master local[4]</span><br></pre></td></tr></table></figure>
<p>或者，可以在CLASSPATH中添加code.jar，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master local[4] --jars code.jar</span><br></pre></td></tr></table></figure>
<p>可以执行“pyspark –help”命令，获取完整的选项列表，具体如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --help</span><br></pre></td></tr></table></figure>
<p>上面是命令使用方法介绍，下面正式使用命令进入pyspark环境，可以通过下面命令启动pyspark环境：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark</span><br></pre></td></tr></table></figure>
<p>该命令省略了参数，这时，系统默认是“bin/pyspark–master local[*]”，也就是说，是采用本地模式运行，并且使用本地所有的CPU核心。</p>
<p>启动pyspark后，就会进入“&gt;&gt;&gt;”命令提示符状态,如下图所示：<br><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-21-27-07.png"></p>
<p>创建 Python 脚本 <code>my_script.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    sc = SparkContext( <span class="string">&#x27;local&#x27;</span>, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">    logFile = <span class="string">&quot;file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md&quot;</span></span><br><span class="line">    logData = sc.textFile(logFile, <span class="number">2</span>).cache()</span><br><span class="line">    numAs = logData.<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: <span class="string">&#x27;a&#x27;</span> <span class="keyword">in</span> line).count()</span><br><span class="line">    numBs = logData.<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: <span class="string">&#x27;b&#x27;</span> <span class="keyword">in</span> line).count()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Lines with a: %s, Lines with b: %s&#x27;</span> % (numAs, numBs))</span><br></pre></td></tr></table></figure>

<h4 id="通过-spark-submit-运行程序"><a href="#通过-spark-submit-运行程序" class="headerlink" title="通过 spark-submit 运行程序"></a>通过 spark-submit 运行程序</h4><p>我们也可以直接将Python脚本通过 spark-submit 提交到 Spark 中运行了，命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class <span class="string">&quot;SimpleApp&quot;</span> SimpleApp.py</span><br></pre></td></tr></table></figure>



<h3 id="Scala-on-Spark独立应用编程"><a href="#Scala-on-Spark独立应用编程" class="headerlink" title="Scala on Spark独立应用编程"></a>Scala on Spark独立应用编程</h3><h4 id="1-安装sbt"><a href="#1-安装sbt" class="headerlink" title="1. 安装sbt"></a>1. 安装sbt</h4><p>sbt是一款Spark用来对scala编写程序进行打包的工具，Spark 中没有自带 sbt，我通过到<a target="_blank" rel="noopener" href="https://www.scala-sbt.org/download.html%E9%80%89%E6%8B%A9[sbt-1.3.8.zip](https://piccolo.link/sbt-1.3.8.zip)%E8%BF%9B%E8%A1%8C%E4%B8%8B%E8%BD%BD%E9%85%8D%E7%BD%AE%EF%BC%8C%E5%B0%86%E4%B8%8B%E8%BD%BD%E5%88%B0%E7%9A%84sbt-1.3.8.zip%E8%A7%A3%E5%8E%8B%E5%88%B0%E6%9F%90%E4%B8%AA%E7%9B%AE%E5%BD%95%E5%B9%B6%E6%B7%BB%E5%8A%A0%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%BD%93%E4%B8%AD%EF%BC%9A">https://www.scala-sbt.org/download.html选择[sbt-1.3.8.zip](https://piccolo.link/sbt-1.3.8.zip)进行下载配置，将下载到的sbt-1.3.8.zip解压到某个目录并添加到环境变量当中：</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SBT</span></span><br><span class="line"><span class="built_in">export</span> SBT_HOME=/Users/vincent/opt/sbt</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SBT_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p>如果在国内网络环境，sbt的网络依赖可能会存在下载阻碍，可以单独配置更换国内源，通过新增<code>~/.sbt/repositories</code>文件，添加如下内容后执行<code>sbt --version</code>查看是否正常：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line">local</span><br><span class="line">aliyun: http://maven.aliyun.com/nexus/content/groups/public/</span><br><span class="line">typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly</span><br><span class="line">sonatype-oss-releases</span><br><span class="line">maven-central</span><br><span class="line">sonatype-oss-snapshots</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(base) ➜  ~ sbt –version<br>sbt version in this project: 1.3.8<br>sbt script version: 1.3.8</p>
</blockquote>
<h4 id="2-Scala编码"><a href="#2-Scala编码" class="headerlink" title="2. Scala编码"></a>2. Scala编码</h4><p>Scala是一种与Java兼容的、面向对象的、函数式的编程语言。Spark更是在Scala中实现的，因此Spark中已经包含了Scala的编译器，可以选择不单独配置Scala环境。 在目录<code>sparksrc/scalasrc/</code>新建一个<code>SimpleApp.scala</code>文件，添加如下内容：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* SimpleApp.scala */</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">val</span> logFile = <span class="string">&quot;file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md&quot;</span> <span class="comment">// Should be some file on your system</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Simple Application&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> logData = sc.textFile(logFile, <span class="number">2</span>).cache()</span><br><span class="line">        <span class="keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="string">&quot;a&quot;</span>)).count()</span><br><span class="line">        <span class="keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="string">&quot;b&quot;</span>)).count()</span><br><span class="line">        println(<span class="string">&quot;Lines with a: %s, Lines with b: %s&quot;</span>.format(numAs, numBs))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>同时新建一个sbt工程文件<code>build.sbt</code>  ：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name := &quot;Simple Project&quot;</span><br><span class="line">version := &quot;1.0&quot;</span><br><span class="line">scalaVersion := &quot;2.11.12&quot;</span><br><span class="line">libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.4.0&quot;</span><br></pre></td></tr></table></figure>

<h4 id="3-使用-sbt-打包-Scala-程序"><a href="#3-使用-sbt-打包-Scala-程序" class="headerlink" title="3. 使用 sbt 打包 Scala 程序"></a>3. 使用 sbt 打包 Scala 程序</h4><p>进入<code>sparksrc/scalasrc/</code>目录，执行<code>sbt package</code>命令将整个应用程序打包成 JAR，如果首次运行会下载对应的依赖包，生成的 jar 包的位于生成的target目录中。</p>
<h4 id="4-通过-spark-submit-运行程序"><a href="#4-通过-spark-submit-运行程序" class="headerlink" title="4. 通过 spark-submit 运行程序"></a>4. 通过 spark-submit 运行程序</h4><p>最后，我们就可以将生成的 jar 包通过 spark-submit 提交到 Spark 中运行了，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spark-submit --class <span class="string">&quot;SimpleApp&quot;</span> ./target/scala-2.11/simple-project_2.11-1.0.jar</span><br><span class="line"><span class="comment"># 上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果</span></span><br><span class="line">spark-submit --class <span class="string">&quot;SimpleApp&quot;</span> ./target/scala-2.11/simple-project_2.11-1.0.jar 2&gt;&amp;1 | grep <span class="string">&quot;Lines with a:&quot;</span></span><br></pre></td></tr></table></figure>

<p>最终得到的结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Lines with a: 61, Lines with b: 30</span><br></pre></td></tr></table></figure>

<p>自此，就完成了我的第一个 Spark Scala应用程序了。</p>
<h3 id="Java-on-Spark独立应用编程"><a href="#Java-on-Spark独立应用编程" class="headerlink" title="Java on Spark独立应用编程"></a>Java on Spark独立应用编程</h3><h4 id="1-安装maven"><a href="#1-安装maven" class="headerlink" title="1. 安装maven"></a>1. 安装maven</h4><p>Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。Spark 中没有自带 Maven，我通过到<a target="_blank" rel="noopener" href="http://maven.apache.org/download.cgi%E9%80%89%E6%8B%A9[">http://maven.apache.org/download.cgi选择[</a> apache-maven-3.6.3-bin.zip](<a target="_blank" rel="noopener" href="http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.zip)%E8%BF%9B%E8%A1%8C%E4%B8%8B%E8%BD%BD%E9%85%8D%E7%BD%AE%EF%BC%8C%E5%B0%86%E4%B8%8B%E8%BD%BD%E5%88%B0%E7%9A%84apache-maven-3.6.3-bin.zip%E8%A7%A3%E5%8E%8B%E5%88%B0%E6%9F%90%E4%B8%AA%E7%9B%AE%E5%BD%95%E5%B9%B6%E6%B7%BB%E5%8A%A0%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%BD%93%E4%B8%AD%EF%BC%9A">http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.zip)进行下载配置，将下载到的apache-maven-3.6.3-bin.zip解压到某个目录并添加到环境变量当中：</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MAVEN</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_HOME=/Users/vincent/opt/maven/apache-maven-3.6.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$MAVEN_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p>同样可以为Maven配置更换国内源加速依赖文件下载，通过新增<code>~/.m2/setting.xml文件</code>，添加如下内容后执行<code>mvn --version</code>查看是否正常：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>(base) ➜  ~ mvn –version<br>Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)<br>Maven home: /Users/vincent/opt/maven/apache-maven-3.6.3<br>Java version: 1.8.0_242, vendor: AdoptOpenJDK, runtime: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre<br>Default locale: en_CN, platform encoding: UTF-8<br>OS name: “mac os x”, version: “10.15.3”, arch: “x86_64”, family: “mac”</p>
</blockquote>
<ol start="2">
<li>Java编码</li>
</ol>
<p>在 ~/sparksrc/javasrc 下建立一个名为 SimpleApp.java 的文件（vim ~/sparksrc/javasrc/SimpleApp.java），添加代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* SimpleApp.java */</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String logFile = <span class="string">&quot;file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md&quot;</span>; <span class="comment">// Should be some file on your system</span></span><br><span class="line">        SparkSession spark = SparkSession.builder().appName(<span class="string">&quot;Simple Application&quot;</span>).getOrCreate();</span><br><span class="line">        Dataset&lt;String&gt; logData = spark.read().textFile(logFile).cache();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> numAs = logData.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="string">&quot;a&quot;</span>)).count();</span><br><span class="line">        <span class="keyword">long</span> numBs = logData.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="string">&quot;b&quot;</span>)).count();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;Lines with a: &quot;</span> + numAs + <span class="string">&quot;, lines with b: &quot;</span> + numBs);</span><br><span class="line"></span><br><span class="line">        spark.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>同时新建一个Maven工程文件<code>pom.xml</code>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>fuhailin.github.io<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>simple-project<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Simple Project<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>jboss<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>JBoss Repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repository.jboss.com/maven2/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span> <span class="comment">&lt;!-- Spark dependency --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span> </span><br></pre></td></tr></table></figure>

<h4 id="3-使用-maven-打包-Java-程序"><a href="#3-使用-maven-打包-Java-程序" class="headerlink" title="3. 使用 maven 打包 Java 程序"></a>3. 使用 maven 打包 Java 程序</h4><p>进入<code>sparksrc/javasrc/</code>目录，执行<code>mvn package</code>命令将整个应用程序打包成 JAR，如果首次运行同样会下载对应的maven依赖包，生成的 jar 包的位于生成的target目录中。</p>
<p><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.00.24-PM.png"></p>
<h4 id="4-通过-spark-submit-运行程序-1"><a href="#4-通过-spark-submit-运行程序-1" class="headerlink" title="4. 通过 spark-submit 运行程序"></a>4. 通过 spark-submit 运行程序</h4><p>最后，可以通过将生成的jar包通过spark-submit提交到Spark中运行，如下命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class <span class="string">&quot;SimpleApp&quot;</span> ./target/simple-project-1.0.jar</span><br><span class="line"><span class="comment"># 上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果</span></span><br><span class="line">spark-submit --class <span class="string">&quot;SimpleApp&quot;</span> ./target/simple-project-1.0.jar 2&gt;&amp;1 | grep <span class="string">&quot;Lines with a&quot;</span></span><br></pre></td></tr></table></figure>

<p>这样我们就完成了Spark伪分布式环境的配置以及Spark中支持的三种编程语言的独立程序测试。</p>
<p>关注我的公众号”赵大寳Note”（ID：StateOfTheArt），回复“<strong>HelloSpark</strong>”下载本文中的Python、Scala、Java全部实例工程。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vZnVoYWlsaW4vT2JqZWN0LVN0b3JhZ2UtU2VydmljZS9yYXcvbWFzdGVyL3dlY2hhdF9jaGFubmVsLnBuZw?x-oss-process=image/format,png" alt="关注公众号赵大寳Note，回复“HelloSpark”下载本文全部代码"></p>
<p>一些学习资料：<br><a target="_blank" rel="noopener" href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html">Spark性能优化指南——基础篇</a><br><a href="PySpark_SQL_Cheat_Sheet_Python.pdf">PySpark_SQL_Cheat_Sheet_Python.pdf</a></p>
<p><strong>References</strong>:</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1709-2/">大数据之Spark入门教程(Python版)|厦门大学数据库</a></li>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/examples.html#">https://spark.apache.org/examples.html#</a></li>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/quick-start.html">https://spark.apache.org/docs/latest/quick-start.html</a></li>
<li><a target="_blank" rel="noopener" href="https://www.journaldev.com/20342/apache-spark-example-word-count-program-java">Apache Spark Example: Word Count Program in Java</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.scala-lang.org/getting-started/">https://docs.scala-lang.org/getting-started/</a></li>
<li><a target="_blank" rel="noopener" href="http://blog.miz.space/tutorial/2016/08/30/how-to-integrate-spark-intellij-idea-and-scala-install-setup-ubuntu-windows-mac/">How to integrate Apache Spark, Intellij Idea and Scala</a></li>
<li><a target="_blank" rel="noopener" href="https://www.runoob.com/scala/scala-intro.html">https://www.runoob.com/scala/scala-intro.html</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>您的支持将鼓励我继续创作！</div>
  <button>
    Donate
  </button>
  <div class="post-reward">
      <div>
        <img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/wechatpay.jpg" alt="赵大寳 WeChat Pay">
        <span>WeChat Pay</span>
      </div>
      <div>
        <img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/alipay.jpg" alt="赵大寳 Alipay">
        <span>Alipay</span>
      </div>
      <div>
        <img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/bitcoinpay.jpg" alt="赵大寳 Bitcoin">
        <span>Bitcoin</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>Welcome to my other publishing channels</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
          <span class="icon">
            <i class="fab fa-weixin"></i>
          </span>

          <span class="label">WeChat</span>
        </a>
      </div>

      <div class="social-item">
        <a target="_blank" class="social-link" href="/atom.xml">
          <span class="icon">
            <i class="fa fa-rss"></i>
          </span>

          <span class="label">RSS</span>
        </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/HDFS-Overview/" rel="prev" title="HDFS学习笔记">
                  <i class="fa fa-chevron-left"></i> HDFS学习笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Program-with-RDD-in-PySpark/" rel="next" title="Spark入门笔记—编程操作对象RDD与DataFrame(PySpark版)">
                  Spark入门笔记—编程操作对象RDD与DataFrame(PySpark版) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC80MjE1MS8xODY5OA=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="snowflake-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">赵大寳</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  




  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>

</body>
</html>
