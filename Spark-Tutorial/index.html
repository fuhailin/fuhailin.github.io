<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/180x180-logo.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/32x32-logo.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/16x16-logo.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"fuhailin.github.io","root":"/","scheme":"Muse","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Spark学习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark入门笔记—基本概念与单机环境配置">
<meta property="og:url" content="https://fuhailin.github.io/Spark-Tutorial/index.html">
<meta property="og:site_name" content="赵大寳">
<meta property="og:description" content="Spark学习笔记">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-11.01.45-PM.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-18-01-05.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.24.52-PM.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-21-27-07.png">
<meta property="og:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.00.24-PM.png">
<meta property="og:image" content="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vZnVoYWlsaW4vT2JqZWN0LVN0b3JhZ2UtU2VydmljZS9yYXcvbWFzdGVyL3dlY2hhdF9jaGFubmVsLnBuZw?x-oss-process=image/format,png">
<meta property="article:published_time" content="2019-04-25T10:25:02.000Z">
<meta property="article:modified_time" content="2020-03-08T16:53:37.031Z">
<meta property="article:author" content="赵大寳">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-11.01.45-PM.png">

<link rel="canonical" href="https://fuhailin.github.io/Spark-Tutorial/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Spark入门笔记—基本概念与单机环境配置 | 赵大寳</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129037882-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-129037882-1');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?d0967e9b160f4f6248804003642ee818";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">赵大寳</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">鶸鸡程序员，新世纪农民工</p>
  </div>

  <div class="site-nav-right"></div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/fuhailin" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="https://fuhailin.github.io/Spark-Tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/uploads-avatar.jpg">
      <meta itemprop="name" content="赵大寳">
      <meta itemprop="description" content="赵大寳個人小站">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="赵大寳">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark入门笔记—基本概念与单机环境配置
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-04-25 18:25:02" itemprop="dateCreated datePublished" datetime="2019-04-25T18:25:02+08:00">2019-04-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-09 00:53:37" itemprop="dateModified" datetime="2020-03-09T00:53:37+08:00">2020-03-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <div class="post-description">Spark学习笔记</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文通过收集Spark中的基本概念、在Mac上配置伪分布式环境并分别用Python、Scala、Java三种语言独立编程实现了分布式版的WordCount程序以进行测试学习，来熟悉Spark的常用操作。</p>
<a id="more"></a>
<h1 id="Spark中的基本概念"><a href="#Spark中的基本概念" class="headerlink" title="Spark中的基本概念"></a>Spark中的基本概念</h1><h2 id="Spark-Shell"><a href="#Spark-Shell" class="headerlink" title="Spark Shell"></a>Spark Shell</h2><p>Spark的shell提供了一个简单的API可供学习, 其也是一个用于分析数据的强有力交互工具。</p>
<h2 id="RDD-Resilient-Distributed-Dataset"><a href="#RDD-Resilient-Distributed-Dataset" class="headerlink" title="RDD(Resilient Distributed Dataset)"></a>RDD(Resilient Distributed Dataset)</h2><p>RDD（Resilient Distributed Dataset）叫做<strong>弹性分布式数据集</strong>，是Spark中最基本的数据结构。它是一个不可变的分布式对象集合。在RDD中的每一个数据集被划分进逻辑分区，不同的部分将在集群的不同节点上进行计算。RDD能够包含任意类型的对象，包括Python、Java、Scala甚至用户自定义类型。</p>
<h2 id="Spark中的组件"><a href="#Spark中的组件" class="headerlink" title="Spark中的组件"></a>Spark中的组件</h2><p>Spark组件使Apache Spark快速和可靠。为了解决使用Hadoop MapReduce时出现的问题，很多Spark组件被构建出来。 Apache Spark具有以下组件：</p>
<ol>
<li>Spark Core</li>
<li>Spark Streaming</li>
<li>Spark SQL</li>
<li>GraphX</li>
<li>MLlib (Machine Learning)</li>
</ol>
<h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h3><p><em>Spark Core</em>是大规模并行计算和分布式数据处理的基本引擎。它负责：</p>
<ol>
<li>内存管理和故障恢复</li>
<li>在群集上调度，分发和监视作业</li>
<li>与存储系统交互</li>
</ol>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p><em>Spark Streaming</em> 是Spark用于处理实时流数据的组件。它支持实时数据流的高吞吐量和容错流处理。基本流单元是 <em>DStream</em> ，其基本上是一系列用于处理实时数据的RDD（弹性分布式数据集）。</p>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p><em>Spark SQL</em> 是Spark中的一个新模块，它将关系处理与Spark的函数式编程API集成在一起。 它支持通过SQL或Hive查询语言查询数据。以下是Spark SQL的四个库：</p>
<ol>
<li>Data Source API</li>
<li>DataFrame API</li>
<li>Interpreter &amp; Optimizer</li>
<li>SQL Service</li>
</ol>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p><em>GraphX</em>是用于图形和图形并行计算的Spark API。 因此，它使用弹性分布式属性图(Resilient Distributed Property Graph)扩展了Spark RDD。</p>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><p><em>MLlib</em> 代表机器学习库Machine Learning Library。Spark MLlib用于在Apache Spark中执行机器学习。</p>
<h1 id="Spark的安装"><a href="#Spark的安装" class="headerlink" title="Spark的安装"></a>Spark的安装</h1><p>Spark可以独立安装使用，也可以和Hadoop一起安装使用。这里我们采用和Hadoop一起安装使用，这样就可以让Spark使用HDFS存取数据。需要说明的是，当安装好Spark以后，里面就自带了Scala环境，不需要额外安装Scala.</p>
<p>本教程的具体运行环境如下：</p>
<ul>
<li>Hadoop 3.1.3</li>
<li>Java JDK 1.8</li>
<li>Spark 2.4.5</li>
</ul>
<h2 id="安装JDK与Hadoop"><a href="#安装JDK与Hadoop" class="headerlink" title="安装JDK与Hadoop"></a>安装JDK与Hadoop</h2><p><a href="https://fuhailin.github.io/Hadoop-on-MacOS/">https://fuhailin.github.io/Hadoop-on-MacOS/</a></p>
<h2 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h2><p>由于已经安装了Hadoop，所以，在“Choose a package type”后面需要选择“Pre-build with user-provided Hadoop [can use with most Hadoop distributions]”，然后，点击“Download Spark”后面的“spark-2.4.5-bin-without-hadoop.tgz”下载即可。<br>Spark部署模式主要有四种：</p>
<ul>
<li>Local模式（单机模式）</li>
<li>Standalone模式（使用Spark自带的简单集群管理器）</li>
<li>YARN模式（使用YARN作为集群管理器）</li>
<li>Mesos模式（使用Mesos作为集群管理器）</li>
</ul>
<p>这里介绍Local模式（单机模式）的 Spark安装。我们选择Spark 2.4.5 版本，并且假设当前使用用户名hadoop登录了Linux操作系统(MacOS可忽略这一步操作)。<br></p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxf spark-2.4.2-bin-without-hadoop.tgz -C /usr/<span class="built_in">local</span>/</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span></span><br><span class="line">sudo mv ./spark-2.4.2-bin-without-hadoop/ ./spark</span><br><span class="line">sudo chown -R hadoop:hadoop ./spark          <span class="comment"># 此处的 hadoop 为你的用户名，MacOS可忽略</span></span><br></pre></td></tr></tbody></table></figure><br>安装后，还需要修改Spark的配置文件spark-env.sh<br><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/spark</span><br><span class="line">cp ./conf/spark-env.sh.template ./conf/spark-env.sh</span><br></pre></td></tr></tbody></table></figure><br>编辑spark-env.sh文件(vim ./conf/spark-env.sh)，添加你的Hadoop配置信息:<br><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br></pre></td></tr></tbody></table></figure><br>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。然后通过如下命令，修改环境变量<code>vim ~/.bashrc</code>，在.bashrc文件中添加如下内容：<br><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SPARK CONFIG</span></span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$SPARK_HOME</span>/python:<span class="variable">$SPARK_HOME</span>/python/lib/py4j-0.10.7-src.zip:<span class="variable">$PYTHONPATH</span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=python3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></tbody></table></figure><br>PYTHONPATH环境变量主要是为了在Python3中引入pyspark库，PYSPARK_PYTHON变量主要是设置pyspark运行的python版本。<br>.bashrc中必须包含<code>JAVA_HOME</code>,<code>HADOOP_HOME</code>,<code>SPARK_HOME</code>,<code>PYTHONPATH</code>,<code>PYSPARK_PYTHON</code>,<code>PATH</code>这些环境变量。如果已经设置了这些变量则不需要重新添加设置。<br><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-11.01.45-PM.png" alt="SPARK_HOME环境变量"><br>接着还需要让该环境变量生效，执行<code>source ~/.bashrc</code>。<br>配置完成后就可以直接使用，不需要像Hadoop运行启动命令。<br>通过运行Spark自带的示例，验证Spark是否安装成功。<p></p>
<figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run-example SparkPi</span><br></pre></td></tr></tbody></table></figure>
<p>执行时会输出非常多的运行信息，输出结果不容易找到，可以通过 grep 命令进行过滤（命令中的 2&gt;&amp;1 可以将所有的信息都输出到 stdout 中，否则由于输出日志的性质，还是会输出到屏幕中）:<br></p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run-example SparkPi 2&gt;&amp;1 | grep <span class="string">"Pi is"</span></span><br></pre></td></tr></tbody></table></figure><br>过滤后的运行结果如下图示，<br><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-18-01-05.png" alt="run-example SparkPi"><p></p>
<h2 id="使用-Spark-Shell-编写代码"><a href="#使用-Spark-Shell-编写代码" class="headerlink" title="使用 Spark Shell 编写代码"></a>使用 Spark Shell 编写代码</h2><p><strong>启动Spark Shell</strong>：spark-shell，启动spark-shell后，会自动创建名为sc的SparkContext对象和名为spark的SparkSession对象,如图：</p>
<p><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.24.52-PM.png" alt="Screen-Shot-2020-03-08-at-10.24.52-PM"></p>
<h3 id="加载text文件"><a href="#加载text文件" class="headerlink" title="加载text文件"></a>加载text文件</h3><p>spark创建sc，可以加载本地文件和HDFS文件创建RDD。这里用Spark自带的本地文件README.md文件测试。</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">"file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md"</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>加载HDFS文件和本地文件都是使用textFile，区别是添加前缀(<code>hdfs://</code>和<code>file:///</code>)进行标识。</p>
<h1 id="PySpark独立应用程序编程"><a href="#PySpark独立应用程序编程" class="headerlink" title="PySpark独立应用程序编程"></a>PySpark独立应用程序编程</h1><p>学习Spark程序开发，建议首先通过pyspark交互式学习，加深Spark程序开发的理解。<br>PySpark提供了简单的方式来学习 API，并且提供了交互的方式来分析数据。你可以输入一条语句，PySpark会立即执行语句并返回结果，这就是我们所说的REPL（Read-Eval-Print Loop，交互式解释器），为我们提供了交互式执行环境，表达式计算完成就会输出结果，而不必等到整个程序运行完毕，因此可即时查看中间结果，并对程序进行修改，这样可以在很大程度上提升开发效率。</p>
<p>前面已经安装了Hadoop和Spark，如果Spark不使用HDFS和YARN，那么就不用启动Hadoop也可以正常使用Spark。如果在使用Spark的过程中需要用到 HDFS，就要首先启动 Hadoop（启动Hadoop的方法可以参考上面给出的<a href="https://fuhailin.github.io/Hadoop-Install/">Hadoop安装教程</a>）。<br>这里假设不需要用到HDFS，因此，就没有启动Hadoop。现在我们直接开始使用Spark。</p>
<p>注意：如果按照上面的安装步骤，已经设置了PYSPARK_PYTHON环境变量，那么你直接使用如下命令启动pyspark即可。<br><code>pyspark</code><br>如果没有设置PYSPARK_PYTHON环境变量，则使用如下命令启动pyspark<br></p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PYSPARK_PYTHON=python3</span><br><span class="line">pyspark</span><br></pre></td></tr></tbody></table></figure><br>pyspark命令及其常用的参数如下：<br><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master &lt;master-url&gt;</span><br></pre></td></tr></tbody></table></figure><br>Spark的运行模式取决于传递给SparkContext的Master URL的值。Master URL可以是以下任一种形式：<p></p>
<ul>
<li>local 使用一个Worker线程本地化运行SPARK(完全不并行)</li>
<li>local[*] 使用逻辑CPU个数数量的线程来本地化运行Spark</li>
<li>local[K] 使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU核数设定）</li>
<li>spark://HOST:PORT 连接到指定的Spark standalone master。默认端口是7077.</li>
<li>yarn-client 以客户端模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</li>
<li>yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到。</li>
<li>mesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050。</li>
</ul>
<p>需要强调的是，这里我们采用“本地模式”（local）运行Spark，关于如何在集群模式下运行Spark，可以参考后面的“在集群上运行Spark应用程序”。<br>在Spark中采用本地模式启动pyspark的命令主要包含以下参数：<br>–master：这个参数表示当前的pyspark要连接到哪个master，如果是local[*]，就是使用本地模式启动pyspark，其中，中括号内的星号表示需要使用几个CPU核心(core)；<br>–jars： 这个参数用于把相关的JAR包添加到CLASSPATH中；如果有多个jar包，可以使用逗号分隔符连接它们；</p>
<p>比如，要采用本地模式，在4个CPU核心上运行pyspark：<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master local[4]</span><br></pre></td></tr></tbody></table></figure><br>或者，可以在CLASSPATH中添加code.jar，命令如下：<br><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --master local[4] --jars code.jar</span><br></pre></td></tr></tbody></table></figure><br>可以执行“pyspark –help”命令，获取完整的选项列表，具体如下：<br><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark --help</span><br></pre></td></tr></tbody></table></figure><br>上面是命令使用方法介绍，下面正式使用命令进入pyspark环境，可以通过下面命令启动pyspark环境：<br><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyspark</span><br></pre></td></tr></tbody></table></figure><br>该命令省略了参数，这时，系统默认是“bin/pyspark–master local[*]”，也就是说，是采用本地模式运行，并且使用本地所有的CPU核心。<p></p>
<p>启动pyspark后，就会进入“&gt;&gt;&gt;”命令提示符状态,如下图所示：<br><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/2019-04-25-21-27-07.png" alt=""></p>
<p>创建 Python 脚本 <code>my_script.py</code>：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    sc = SparkContext( <span class="string">'local'</span>, <span class="string">'test'</span>)</span><br><span class="line">    logFile = <span class="string">"file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md"</span></span><br><span class="line">    logData = sc.textFile(logFile, <span class="number">2</span>).cache()</span><br><span class="line">    numAs = logData.filter(<span class="keyword">lambda</span> line: <span class="string">'a'</span> <span class="keyword">in</span> line).count()</span><br><span class="line">    numBs = logData.filter(<span class="keyword">lambda</span> line: <span class="string">'b'</span> <span class="keyword">in</span> line).count()</span><br><span class="line">    print(<span class="string">'Lines with a: %s, Lines with b: %s'</span> % (numAs, numBs))</span><br></pre></td></tr></tbody></table></figure>
<h4 id="通过-spark-submit-运行程序"><a href="#通过-spark-submit-运行程序" class="headerlink" title="通过 spark-submit 运行程序"></a>通过 spark-submit 运行程序</h4><p>我们也可以直接将Python脚本通过 spark-submit 提交到 Spark 中运行了，命令如下：</p>
<figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class <span class="string">"SimpleApp"</span> SimpleApp.py</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Scala-on-Spark独立应用编程"><a href="#Scala-on-Spark独立应用编程" class="headerlink" title="Scala on Spark独立应用编程"></a>Scala on Spark独立应用编程</h3><h4 id="1-安装sbt"><a href="#1-安装sbt" class="headerlink" title="1. 安装sbt"></a>1. 安装sbt</h4><p>sbt是一款Spark用来对scala编写程序进行打包的工具，Spark 中没有自带 sbt，我通过到<a href="https://www.scala-sbt.org/download.html选择[sbt-1.3.8.zip](https://piccolo.link/sbt-1.3.8.zip)进行下载配置，将下载到的sbt-1.3.8.zip解压到某个目录并添加到环境变量当中：" target="_blank" rel="noopener">https://www.scala-sbt.org/download.html选择[sbt-1.3.8.zip](https://piccolo.link/sbt-1.3.8.zip)进行下载配置，将下载到的sbt-1.3.8.zip解压到某个目录并添加到环境变量当中：</a></p>
<figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SBT</span></span><br><span class="line"><span class="built_in">export</span> SBT_HOME=/Users/vincent/opt/sbt</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SBT_HOME</span>/bin</span><br></pre></td></tr></tbody></table></figure>
<p>如果在国内网络环境，sbt的网络依赖可能会存在下载阻碍，可以单独配置更换国内源，通过新增<code>~/.sbt/repositories</code>文件，添加如下内容后执行<code>sbt --version</code>查看是否正常：</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line">local</span><br><span class="line">aliyun: http://maven.aliyun.com/nexus/content/groups/public/</span><br><span class="line">typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly</span><br><span class="line">sonatype-oss-releases</span><br><span class="line">maven-central</span><br><span class="line">sonatype-oss-snapshots</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>(base) ➜  ~ sbt —version<br>sbt version in this project: 1.3.8<br>sbt script version: 1.3.8</p>
</blockquote>
<h4 id="2-Scala编码"><a href="#2-Scala编码" class="headerlink" title="2. Scala编码"></a>2. Scala编码</h4><p>Scala是一种与Java兼容的、面向对象的、函数式的编程语言。Spark更是在Scala中实现的，因此Spark中已经包含了Scala的编译器，可以选择不单独配置Scala环境。 在目录<code>sparksrc/scalasrc/</code>新建一个<code>SimpleApp.scala</code>文件，添加如下内容：</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* SimpleApp.scala */</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) {</span><br><span class="line">        <span class="keyword">val</span> logFile = <span class="string">"file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md"</span> <span class="comment">// Should be some file on your system</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Simple Application"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> logData = sc.textFile(logFile, <span class="number">2</span>).cache()</span><br><span class="line">        <span class="keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="string">"a"</span>)).count()</span><br><span class="line">        <span class="keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="string">"b"</span>)).count()</span><br><span class="line">        println(<span class="string">"Lines with a: %s, Lines with b: %s"</span>.format(numAs, numBs))</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>同时新建一个sbt工程文件<code>build.sbt</code>  ：</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name := "Simple Project"</span><br><span class="line">version := "1.0"</span><br><span class="line">scalaVersion := "2.11.12"</span><br><span class="line">libraryDependencies += "org.apache.spark" %% "spark-core" % "2.4.0"</span><br></pre></td></tr></tbody></table></figure>
<h4 id="3-使用-sbt-打包-Scala-程序"><a href="#3-使用-sbt-打包-Scala-程序" class="headerlink" title="3. 使用 sbt 打包 Scala 程序"></a>3. 使用 sbt 打包 Scala 程序</h4><p>进入<code>sparksrc/scalasrc/</code>目录，执行<code>sbt package</code>命令将整个应用程序打包成 JAR，如果首次运行会下载对应的依赖包，生成的 jar 包的位于生成的target目录中。</p>
<h4 id="4-通过-spark-submit-运行程序"><a href="#4-通过-spark-submit-运行程序" class="headerlink" title="4. 通过 spark-submit 运行程序"></a>4. 通过 spark-submit 运行程序</h4><p>最后，我们就可以将生成的 jar 包通过 spark-submit 提交到 Spark 中运行了，命令如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spark-submit --class <span class="string">"SimpleApp"</span> ./target/scala-2.11/simple-project_2.11-1.0.jar</span><br><span class="line"><span class="comment"># 上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果</span></span><br><span class="line">spark-submit --class <span class="string">"SimpleApp"</span> ./target/scala-2.11/simple-project_2.11-1.0.jar 2&gt;&amp;1 | grep <span class="string">"Lines with a:"</span></span><br></pre></td></tr></tbody></table></figure>
<p>最终得到的结果如下：</p>
<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Lines with a: 61, Lines with b: 30</span><br></pre></td></tr></tbody></table></figure>
<p>自此，就完成了我的第一个 Spark Scala应用程序了。</p>
<h3 id="Java-on-Spark独立应用编程"><a href="#Java-on-Spark独立应用编程" class="headerlink" title="Java on Spark独立应用编程"></a>Java on Spark独立应用编程</h3><h4 id="1-安装maven"><a href="#1-安装maven" class="headerlink" title="1. 安装maven"></a>1. 安装maven</h4><p>Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。Spark 中没有自带 Maven，我通过到<a href="http://maven.apache.org/download.cgi选择[" target="_blank" rel="noopener">http://maven.apache.org/download.cgi选择[</a> apache-maven-3.6.3-bin.zip](<a href="http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.zip)进行下载配置，将下载到的apache-maven-3.6.3-bin.zip解压到某个目录并添加到环境变量当中：" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.zip)进行下载配置，将下载到的apache-maven-3.6.3-bin.zip解压到某个目录并添加到环境变量当中：</a></p>
<figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MAVEN</span></span><br><span class="line"><span class="built_in">export</span> MAVEN_HOME=/Users/vincent/opt/maven/apache-maven-3.6.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$MAVEN_HOME</span>/bin</span><br></pre></td></tr></tbody></table></figure>
<p>同样可以为Maven配置更换国内源加速依赖文件下载，通过新增<code>~/.m2/setting.xml文件</code>，添加如下内容后执行<code>mvn --version</code>查看是否正常：</p>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>(base) ➜  ~ mvn —version<br>Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)<br>Maven home: /Users/vincent/opt/maven/apache-maven-3.6.3<br>Java version: 1.8.0_242, vendor: AdoptOpenJDK, runtime: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre<br>Default locale: en_CN, platform encoding: UTF-8<br>OS name: “mac os x”, version: “10.15.3”, arch: “x86_64”, family: “mac”</p>
</blockquote>
<ol>
<li>Java编码</li>
</ol>
<p>在 ~/sparksrc/javasrc 下建立一个名为 SimpleApp.java 的文件（vim ~/sparksrc/javasrc/SimpleApp.java），添加代码如下：</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* SimpleApp.java */</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FilterFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Dataset;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleApp</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>{</span><br><span class="line">        String logFile = <span class="string">"file:///Users/vincent/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/README.md"</span>; <span class="comment">// Should be some file on your system</span></span><br><span class="line">        SparkSession spark = SparkSession.builder().appName(<span class="string">"Simple Application"</span>).getOrCreate();</span><br><span class="line">        Dataset&lt;String&gt; logData = spark.read().textFile(logFile).cache();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> numAs = logData.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="string">"a"</span>)).count();</span><br><span class="line">        <span class="keyword">long</span> numBs = logData.filter((FilterFunction&lt;String&gt;) s -&gt; s.contains(<span class="string">"b"</span>)).count();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"Lines with a: "</span> + numAs + <span class="string">", lines with b: "</span> + numBs);</span><br><span class="line"></span><br><span class="line">        spark.stop();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>同时新建一个Maven工程文件<code>pom.xml</code>：</p>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>fuhailin.github.io<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>simple-project<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Simple Project<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>jboss<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>JBoss Repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repository.jboss.com/maven2/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span> <span class="comment">&lt;!-- Spark dependency --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="3-使用-maven-打包-Java-程序"><a href="#3-使用-maven-打包-Java-程序" class="headerlink" title="3. 使用 maven 打包 Java 程序"></a>3. 使用 maven 打包 Java 程序</h4><p>进入<code>sparksrc/javasrc/</code>目录，执行<code>mvn package</code>命令将整个应用程序打包成 JAR，如果首次运行同样会下载对应的maven依赖包，生成的 jar 包的位于生成的target目录中。</p>
<p><img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/hadoop/Screen-Shot-2020-03-08-at-10.00.24-PM.png" alt=""></p>
<h4 id="4-通过-spark-submit-运行程序-1"><a href="#4-通过-spark-submit-运行程序-1" class="headerlink" title="4. 通过 spark-submit 运行程序"></a>4. 通过 spark-submit 运行程序</h4><p>最后，可以通过将生成的jar包通过spark-submit提交到Spark中运行，如下命令：</p>
<figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class <span class="string">"SimpleApp"</span> ./target/simple-project-1.0.jar</span><br><span class="line"><span class="comment"># 上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果</span></span><br><span class="line">spark-submit --class <span class="string">"SimpleApp"</span> ./target/simple-project-1.0.jar 2&gt;&amp;1 | grep <span class="string">"Lines with a"</span></span><br></pre></td></tr></tbody></table></figure>
<p>这样我们就完成了Spark伪分布式环境的配置以及Spark中支持的三种编程语言的独立程序测试。</p>
<p>关注我的公众号”赵大寳Note”（ID：StateOfTheArt），回复“<strong>HelloSpark</strong>”下载本文中的Python、Scala、Java全部实例工程。<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vZnVoYWlsaW4vT2JqZWN0LVN0b3JhZ2UtU2VydmljZS9yYXcvbWFzdGVyL3dlY2hhdF9jaGFubmVsLnBuZw?x-oss-process=image/format,png" alt="关注公众号赵大寳Note，回复“HelloSpark”下载本文全部代码"></p>
<p>一些学习资料：<br><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">Spark性能优化指南——基础篇</a><br><a href="PySpark_SQL_Cheat_Sheet_Python.pdf">PySpark_SQL_Cheat_Sheet_Python.pdf</a></p>
<p><strong>References</strong>:</p>
<ol>
<li><a href="http://dblab.xmu.edu.cn/blog/1709-2/" target="_blank" rel="noopener">大数据之Spark入门教程(Python版)|厦门大学数据库</a></li>
<li><a href="https://spark.apache.org/examples.html#" target="_blank" rel="noopener">https://spark.apache.org/examples.html#</a></li>
<li><a href="https://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/quick-start.html</a></li>
<li><a href="https://www.journaldev.com/20342/apache-spark-example-word-count-program-java" target="_blank" rel="noopener">Apache Spark Example: Word Count Program in Java</a></li>
<li><a href="https://docs.scala-lang.org/getting-started/" target="_blank" rel="noopener">https://docs.scala-lang.org/getting-started/</a></li>
<li><a href="http://blog.miz.space/tutorial/2016/08/30/how-to-integrate-spark-intellij-idea-and-scala-install-setup-ubuntu-windows-mac/" target="_blank" rel="noopener">How to integrate Apache Spark, Intellij Idea and Scala</a></li>
<li><a href="https://www.runoob.com/scala/scala-intro.html" target="_blank" rel="noopener">https://www.runoob.com/scala/scala-intro.html</a></li>
</ol>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
    </div>

    
    
    
        <div class="reward-container">
  <div>您的支持将鼓励我继续创作！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/wechatpay.jpg" alt="赵大寳 WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/alipay.jpg" alt="赵大寳 Alipay">
        <p>Alipay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/bitcoinpay.jpg" alt="赵大寳 Bitcoin">
        <p>Bitcoin</p>
      </div>

  </div>
</div>

        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

            <div class="social-item">
              <a target="_blank" class="social-link" href="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/wechat_channel.png">
                <span class="icon">
                  <i class="fa fa-wechat"></i>
                </span>

                <span class="label">微信搜一搜：赵大寳Note</span>
              </a>
            </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/HDFS-Overview/" rel="prev" title="HDFS学习笔记">
      <i class="fa fa-chevron-left"></i> HDFS学习笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/Program-with-RDD-in-PySpark/" rel="next" title="Spark入门笔记—编程操作对象RDD与DataFrame(PySpark版)">
      Spark入门笔记—编程操作对象RDD与DataFrame(PySpark版) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80MjE1MS8xODY5OA=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark中的基本概念"><span class="nav-number">1.</span> <span class="nav-text">Spark中的基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-Shell"><span class="nav-number">1.1.</span> <span class="nav-text">Spark Shell</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD-Resilient-Distributed-Dataset"><span class="nav-number">1.2.</span> <span class="nav-text">RDD(Resilient Distributed Dataset)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark中的组件"><span class="nav-number">1.3.</span> <span class="nav-text">Spark中的组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Core"><span class="nav-number">1.3.1.</span> <span class="nav-text">Spark Core</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">1.3.2.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">1.3.3.</span> <span class="nav-text">Spark SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GraphX"><span class="nav-number">1.3.4.</span> <span class="nav-text">GraphX</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLlib"><span class="nav-number">1.3.5.</span> <span class="nav-text">MLlib</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark的安装"><span class="nav-number">2.</span> <span class="nav-text">Spark的安装</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#安装JDK与Hadoop"><span class="nav-number">2.1.</span> <span class="nav-text">安装JDK与Hadoop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装Spark"><span class="nav-number">2.2.</span> <span class="nav-text">安装Spark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用-Spark-Shell-编写代码"><span class="nav-number">2.3.</span> <span class="nav-text">使用 Spark Shell 编写代码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#加载text文件"><span class="nav-number">2.3.1.</span> <span class="nav-text">加载text文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PySpark独立应用程序编程"><span class="nav-number">3.</span> <span class="nav-text">PySpark独立应用程序编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#通过-spark-submit-运行程序"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">通过 spark-submit 运行程序</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scala-on-Spark独立应用编程"><span class="nav-number">3.0.1.</span> <span class="nav-text">Scala on Spark独立应用编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-安装sbt"><span class="nav-number">3.0.1.1.</span> <span class="nav-text">1. 安装sbt</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Scala编码"><span class="nav-number">3.0.1.2.</span> <span class="nav-text">2. Scala编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-使用-sbt-打包-Scala-程序"><span class="nav-number">3.0.1.3.</span> <span class="nav-text">3. 使用 sbt 打包 Scala 程序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-通过-spark-submit-运行程序"><span class="nav-number">3.0.1.4.</span> <span class="nav-text">4. 通过 spark-submit 运行程序</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Java-on-Spark独立应用编程"><span class="nav-number">3.0.2.</span> <span class="nav-text">Java on Spark独立应用编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-安装maven"><span class="nav-number">3.0.2.1.</span> <span class="nav-text">1. 安装maven</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-使用-maven-打包-Java-程序"><span class="nav-number">3.0.2.2.</span> <span class="nav-text">3. 使用 maven 打包 Java 程序</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-通过-spark-submit-运行程序-1"><span class="nav-number">3.0.2.3.</span> <span class="nav-text">4. 通过 spark-submit 运行程序</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="赵大寳"
      src="https://gitee.com/fuhailin/Object-Storage-Service/raw/master/uploads-avatar.jpg">
  <p class="site-author-name" itemprop="name">赵大寳</p>
  <div class="site-description" itemprop="description">赵大寳個人小站</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">77</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/fuhailin" title="GitHub → https://github.com/fuhailin" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hailinfufu@outlook.com" title="E-Mail → mailto:hailinfufu@outlook.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/%E6%B5%B7%E6%9E%97-%E4%BB%98-855633ab/" title="Linkedin → https://www.linkedin.com/in/%E6%B5%B7%E6%9E%97-%E4%BB%98-855633ab/" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i>Linkedin</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/u010412858" title="CSDN → https://blog.csdn.net/u010412858" rel="noopener" target="_blank"><i class="fa fa-fw fa-csdn"></i>CSDN</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://tcxx.info/" title="https://tcxx.info/" rel="noopener" target="_blank">甜欣屋</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://licstar.net/" title="http://licstar.net/" rel="noopener" target="_blank">Siwei Lai的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wepon.me/" title="http://wepon.me/" rel="noopener" target="_blank">wepon的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://jd92.wang/" title="http://jd92.wang/" rel="noopener" target="_blank">迁移学习王晋东</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.yinwang.org/" title="http://www.yinwang.org/" rel="noopener" target="_blank">王垠的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://blog.sina.com.cn/weiyanzheng" title="http://blog.sina.com.cn/weiyanzheng" rel="noopener" target="_blank">魏延政的博客</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://iphysresearch.github.io/" title="https://iphysresearch.github.io/" rel="noopener" target="_blank">IPhysResearch</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://paperswithcode.com/task/recommendation-systems" title="https://paperswithcode.com/task/recommendation-systems" rel="noopener" target="_blank">Papers with Code</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://jcf94.com/" title="http://jcf94.com/" rel="noopener" target="_blank">Chenfan Blog</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://coolshell.cn/" title="https://coolshell.cn/" rel="noopener" target="_blank">左耳朵耗子</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-snowflake-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">赵大寳</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.1
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  















  

  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
